# -*- coding: utf-8 -*-
"""CFD1Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G9Bl0Ofmsxmjyo_Rby2It6mMwWeDynmw
"""

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns 
import random
from sklearn.utils import shuffle

data=pd.read_csv("creditcard.csv")
# sns.distplot(data['Amount'])
# sns.distplot(data['Time'])
#
# data.hist(figsize=(20,20))
# plt.show()
#
# sns.jointplot(x= 'Time', y= 'Amount', data= data)
d=data

class0 = d[d['Class']==0]

len(class0)

class1 = d[d['Class']==1]

len(class1)

temp = shuffle(class0)

d1 = temp.iloc[:2000,:]
d1

frames = [d1, class1]
df_temp = pd.concat(frames)

df_temp.info()

df= shuffle(df_temp)

df.to_csv('creditcardsampling.csv')

# sns.countplot('Class', data=df)

"""# SMOTE"""


import imblearn

from imblearn.over_sampling import  SMOTE
oversample=SMOTE()
X=df.iloc[ : ,:-1]
Y=df.iloc[: , -1]
X,Y=oversample.fit_resample(X,Y)

X=pd.DataFrame(X)
X.shape

Y=pd.DataFrame(Y)
Y.head()

names=['Time','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','Amount','Class']

data=pd.concat([X,Y],axis=1)
d=data.values

data.describe()
data.info()

data=pd.DataFrame(d,columns=names)
# sns.countplot('Class', data=data)
#
# plt.figure(figsize=(12,10))
# sns.heatmap(data.corr())

import math
import sklearn.preprocessing

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score , classification_report, confusion_matrix, precision_recall_curve, f1_score, auc

X_train, X_test, y_train, y_test = train_test_split(data.drop('Class', axis=1), data['Class'], test_size=0.3, random_state=42)

"""# Feature Scaling"""

# cols= ['V22', 'V24', 'V25', 'V26', 'V27', 'V28']

scaler = StandardScaler()
frames= ['Time', 'Amount']
x= data[frames]
d_temp = data.drop(frames, axis=1)
temp_col=scaler.fit_transform(x)

scaled_col = pd.DataFrame(temp_col, columns=frames)
scaled_col.head()

d_scaled = pd.concat([scaled_col, d_temp], axis =1)
d_scaled.head()

y = data['Class']
d_scaled.head()

"""# Dimensionality Reduction"""

from sklearn.decomposition import PCA

pca = PCA(n_components=11)
X_temp_reduced = pca.fit_transform(d_scaled)
pca.explained_variance_ratio_
pca.explained_variance_



names=['Name','Phoneno','Lat','Long','Gender','Transaction Id','Platform','Time','Amount','Bank','Reason']

X_reduced= pd.DataFrame(X_temp_reduced,columns=names)
X_reduced.head()

Y=d_scaled['Class']

new_data=pd.concat([X_reduced,Y],axis=1)
new_data.head()
new_data.shape

new_data.to_csv('finaldata.csv')

X_train, X_test, y_train, y_test= train_test_split(X_reduced, d_scaled['Class'], test_size = 0.30, random_state = 42)

X_train.shape, X_test.shape

from sklearn.metrics import classification_report,confusion_matrix

"""# Support Vector Machine"""

from sklearn.svm import SVC
svc=SVC(kernel='rbf',probability=True)
svc.fit(X_train,y_train)
y_pred_svc=svc.predict(X_test)
y_pred_svc

type(X_test)
X_test.to_csv('testing.csv')

from sklearn.model_selection import GridSearchCV
parameters = [ {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 1, 0.01, 0.0001 ,0.001]}]
grid_search = GridSearchCV(estimator = svc,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           n_jobs = -1)
grid_search = grid_search.fit(X_train, y_train)
best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
print("Best Accuracy: {:.2f} %".format(best_accuracy*100))
print("Best Parameters:", best_parameters)

svc_param=SVC(kernel='rbf',gamma=0.01,C=100)
svc_param.fit(X_train,y_train)

import pickle
# Saving model to disk

pickle.dump(svc_param, open('cfd_model.pkl','wb'))

model=pickle.load(open('cfd_model.pkl','rb'))

to_predict_list=[6.5419339216889405,-9.945185827720579,4.346044626506952,-4.543737944761002,3.900027175302794,4.886967957765091,-2.5934487206676735,2.8358167476744702,-0.026483007027170657,-1.4365690591572822,3.917490444156668]
to_predict = np.array(to_predict_list).reshape(1, 11)

result=model.predict(to_predict)
result[0]

